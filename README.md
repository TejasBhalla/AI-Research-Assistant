ğŸ¤– AI Research Assistant

AI-Driven Full-Stack Application using Retrieval-Augmented Generation (RAG)

ğŸ“Œ Overview

The AI Research Assistant is an AI-driven full-stack web application that enables users to upload documents and ask natural language questions to receive accurate, context-aware answers with source citations.

The system is built using a Retrieval-Augmented Generation (RAG) architecture, combining semantic search with AI-based answer generation to ensure responses remain grounded in the uploaded documents.

This project was developed during an AI-Driven Full-Stack Development Internship at IGDTUW (Juneâ€“July 2025).

ğŸ§  Key Concepts Used

Retrieval-Augmented Generation (RAG)

Semantic Search

Vector Databases

Document Chunking & Embeddings

AI-Driven Full-Stack Architecture

âœ¨ Features

ğŸ“„ Upload multiple documents (PDFs, notes, research papers)

âœ‚ï¸ Automatic document chunking

ğŸ§¬ Semantic embeddings using Sentence Transformers

ğŸ“¦ Vector storage and retrieval using ChromaDB

ğŸ” Natural language question answering

ğŸ“š Citation-based answers grounded in source documents

ğŸ¨ Modern, animated UI with responsive design

âš¡ Fast and scalable backend using FastAPI

ğŸ—ï¸ System Architecture (RAG Pipeline)
User Question
     â†“
Query Embedding
     â†“
Vector Similarity Search (ChromaDB)
     â†“
Retrieve Top-K Relevant Chunks
     â†“
Context Augmentation
     â†“
AI-Generated Answer with Citations

ğŸ› ï¸ Tech Stack
Frontend

React

Tailwind CSS

Framer Motion

Zustand (state management)

Lucide Icons

Backend

FastAPI

SentenceTransformers

ChromaDB

Pydantic

Uvicorn

ğŸ“‚ Project Structure
ai-research-assistant/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/        # Home, Upload, Ask pages
â”‚   â”‚   â”œâ”€â”€ components/   # UI components
â”‚   â”‚   â”œâ”€â”€ store/        # Zustand stores
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ embedingmodel.py  # FastAPI backend
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ README.md

âš™ï¸ Backend Setup (FastAPI)
1ï¸âƒ£ Create virtual environment
python -m venv venv

2ï¸âƒ£ Activate virtual environment

Windows

venv\Scripts\activate


macOS / Linux

source venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install fastapi uvicorn sentence-transformers chromadb pydantic

4ï¸âƒ£ Run backend server
uvicorn embedingmodel:app --reload


Backend will be available at:

http://127.0.0.1:8000


Swagger UI:

http://127.0.0.1:8000/docs

âš™ï¸ Frontend Setup (React)
1ï¸âƒ£ Install dependencies
npm install

2ï¸âƒ£ Start development server
npm run dev


Frontend runs at:

http://localhost:5173

ğŸ”Œ API Endpoints
â¤ Embed Documents
POST /embed

â¤ Query Documents
POST /query

â¤ Reset Collection
POST /reset

ğŸ“Š Example Query Flow

Upload documents

Documents are chunked & embedded

Embeddings stored in ChromaDB

User asks a question

Relevant chunks retrieved via vector similarity

AI generates an answer using retrieved context

Sources are returned with the answer